{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the accuracy metrics associated to a  (218776, 59, 1) prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the script is located in the same directory as the 'results' folder\n",
    "folder_path = os.path.join('CRN', 'results')\n",
    "file_name = 'new_cancer_sim_2_2.p'\n",
    "\n",
    "path = os.path.join(folder_path, file_name)\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218776, 59, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read npy tile\n",
    "tile = np.load('CRN\\predictions_CRN.npy')\n",
    "tile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218776, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test_data']['cancer_volume'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ..., 59., 59., 59.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test_data']['sequence_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8235676720373158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74      5216\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.69      0.00        13\n",
      "           3       0.98      0.82      0.89     48582\n",
      "\n",
      "    accuracy                           0.82     53811\n",
      "   macro avg       0.41      0.58      0.41     53811\n",
      "weighted avg       0.95      0.82      0.88     53811\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idormoy\\anaconda3\\envs\\RA\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\idormoy\\anaconda3\\envs\\RA\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\idormoy\\anaconda3\\envs\\RA\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4262,    10,     0,   944],\n",
       "       [    0,     0,     0,     0],\n",
       "       [    4,     0,     9,     0],\n",
       "       [ 2079,    52,  6405, 40046]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add counterfactuals benchmaring\n",
    "# We just do the accuracy at each timestep and then we average the accuracy over all the timesteps\n",
    "\n",
    "tests_num = len(results['test_data']['sequence_lengths']) // 4\n",
    "assert len(results['test_data']['sequence_lengths']) % 4 == 0\n",
    "right_answers = 0\n",
    "\n",
    "y_predicted_list = []\n",
    "y_true_list = []\n",
    "\n",
    "for i in range(0, tests_num*4 , 4):\n",
    "    assert results['test_data']['sequence_lengths'][i] == results['test_data']['sequence_lengths'][i+1]\n",
    "    assert results['test_data']['sequence_lengths'][i] == results['test_data']['sequence_lengths'][i+2]\n",
    "    assert results['test_data']['sequence_lengths'][i] == results['test_data']['sequence_lengths'][i+3]\n",
    "\n",
    "    sequence_length = int(results['test_data']['sequence_lengths'][i])\n",
    "    \n",
    "    if sequence_length != 59:\n",
    "        y_predicted = [tile[i][sequence_length], tile[i+1][sequence_length], tile[i+2][sequence_length], tile[i+3][sequence_length]]\n",
    "        min_predicted_index = np.argmin(y_predicted)\n",
    "\n",
    "        # get the index of the true min value\n",
    "        y_true = [results['test_data']['cancer_volume'][i][sequence_length], results['test_data']['cancer_volume'][i+1][sequence_length], \n",
    "                results['test_data']['cancer_volume'][i+2][sequence_length], results['test_data']['cancer_volume'][i+3][sequence_length]]\n",
    "        min_true_index = np.argmin(y_true)\n",
    "\n",
    "        y_predicted_list.append(min_predicted_index)\n",
    "        y_true_list.append(min_true_index)\n",
    "\n",
    "        if min_predicted_index == min_true_index:\n",
    "            right_answers += 1\n",
    "\n",
    "\n",
    "\n",
    "y_predicted_list = np.array(y_predicted_list)\n",
    "y_true_list = np.array(y_true_list)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy: ', accuracy_score(y_true_list, y_predicted_list))\n",
    "\n",
    "# precision, recall, f1-score, support\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true_list, y_predicted_list))\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix(y_true_list, y_predicted_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
